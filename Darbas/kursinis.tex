\documentclass{VUMIFInfKursinis}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{color}
% \usepackage{hyperref}  % Nuorodų aktyvavimas
\usepackage{url}


% Titulinio aprašas
\university{Vilniaus universitetas}
\faculty{Matematikos ir informatikos fakultetas}
\department{Programų sistemų katedra}
\papertype{Kursinis darbas}
\title{Objektų atpažinimas ir sekimas kompiuterinės tomografijos vaizduose}
\titleineng{Object detection and tracking in computed tomography}
\status{3 kurso Programų sistemų studentas}
\author{Paulius Milmantas}
\supervisor{Linas Petkevičius}
\date{Vilnius \\ \the\year}

% Nustatymai
% \setmainfont{Palemonas}   % Pakeisti teksto šriftą į Palemonas (turi būti įdiegtas sistemoje)
\bibliography{bibliografija}

\begin{document}
\maketitle

\sectionnonum{Įvadas}
Pastaraisiais metais į medicinos sritį labai sparčiai skerbiasi informacinės technologijos. Atliekant
įvairias diagnostikas ar tiriant ligas, vaistus yra pasitelkiama programinių sistemų pagalba. Tačiau
didžiają radiologo darbo dalį sprendimus turi priimti jis pats be jokios programinės įrangos pagalbos,
nors dažniausiai yra apibrėžti tam tikri ligų aptikimų algorimtai, kaip vėžinių lastelių aptikimas,
lūžiai ir taip toliau. Mano darbo tikslas yra prisidėti prie programinės įrangos kūrimo radiologams
ir sukurti programą, kuri aptinka plaučius, kad vėliau tai galėtų būti panaudota tolimesnei
IT plėtrai medicinos srityje.
\par
Šiandien viena iš labiausiai perspektyvių sričių informacinių technologijų srityje yra giliojo mokymosi
metodai.
Jie padeda duomenyse aptikti sunkiai pastebimus dėsningumus ir nuspėti išeities rezultatus su tam tikra klaidos tikimybe.
Šiame darbe kalbėsiu vieną iš šių metodų: dirbtinius neuroninius tinklus.
Juos pasirinkau nes kompiuterinėsje tomografijos vaizdai turi labai daug informacijos ir juos apdoruoti
naudojant logines taisykles yra per daug sunku. Tačiau net ir jiems ši užduotis yra per sudėtinga, jeigu
tiriamos yra įvairios mutacijos arba nedažnos ligos, nes su kiekvienu nauju nematytu ligos atveju,
yra reikalinga apmokyti modelį, o tai reikalauja daug duomenų.
\par
%https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248
Pasirinktam mano dirbtinių neuroninių tinklų metodui viena iš galimų alternatyvų yra "sprendimo medžiai".
Tai yra prižiūrimo mokymosi metodas skirtas duomenų klasifikacijai ir duomenų regresijai. Šis metodas
naudoja taisyklių rinkinį. Norint gauti mažą šio metodo paklaidą, turime aprašyti vis daugiau šių taisyklių.
Taisyklės paprastai būna aprašomos naudojant "if, else" aprašus. Mano nagrinėjamame atvejyje duomenys
yra labai dideli ir įvairūs, todėl aprašyti visas šias taisykles užtruktų neproporcingai daug laiko ir tam
tikrų radiologijos žinių.

\tableofcontents

\section{Dirbtinio neuroninio tinklo sudėtis}
\subsection{Bazinė struktūra}
Daugelį dirbtinių neuronų tinklų sudaro panašios struktūrinės dalys:
\begin{enumerate}
  \item Įvesties sluoksnis: tai dalis kuri priima įvestį ir perduoda kitiems sluoksniams.
  \item Išvesties sluoksnis: tai dalis, kuri naudoja aktyvacijos funkciją kuri grąžina galutinį tinklo rezultatą: tikimybių rinkinį, kuris parodo kokia tikimybė, kad objektas atitinka tam tikrą klasę.
  \item Paslėptas sluoksnis: perduoda svorius iš praeito sluoksnio į sekantį.
  \item Susijungimai ir svoriai: tarp kiekvieno neurono, kuris yra susijungęs, turi savo svorį, pagal kurį yra pakeičiama perduodama reikšmė.
  \item Aktyvacijos funkcija: tai funkcija, kokia turi būti neurono išvestis.
  \item Mokymosi taisyklė: apibrėžia, kaip tinkle keičiasi svoriai, kad tinklas išvestų norimus rezultatus.
\end{enumerate}
% https://deepai.org/machine-learning-glossary-and-terms/weight-artificial-neural-network
\par
Dirbtinį neuroninį tinklą taip pat sudaro svoriai ir tendenciškumas. Šie du parametrai yra
tinklo automatiškai sugeneruojami ir apmokant tinklą pagal duomenis, svoriai ir tendenciškumas
yra automatiškai tinklo keičiami. Tendenciškumas apibrėžia kaip tinklo išvestis yra nutolusi
nuo tikrosios reikšmės. Tendenciškumas priartina tinklo išvestį prie tikrųjų reikšmių. Svoriai
nurodo koks stiprus yra ryšys tarp neuronų, taip nurodant, kaip tinklo reikšmės kinta pačiame tinkle.
Jeigu tinklui yra paduodama reikšmė, o pirmasis tinklo neuronas, kuris priima reikšmę turi mažą
svorį, tai reiškia, jog galutiniams rezultatui ši reikšmė neturi daug įtakos. Jeigu svoris yra
didelis, tai duota reikšmė turi daug įtakos rezutatui.
\par
Svoriai ir tendenciškumas yra tinklo tvarkomi treniravimo proceso metu.
prižiūrimasis tinklo treniravimas vyksta dviem žingsniai: priekine ir atgaline sklaida.

\subsection{GPU naudojimas}
\par
Šiame darbe bus naudojamas GPU, atliekant tinklo treniravimą. Tai yra spartesnis būdas už CPU
naudojimas, nes CPU turi kelis galingus branduolius, o GPU žymiai daugiau. Giliajame mokyme,
naudojami aritmetiniai veiksmai yra ganėtinai paprasti, todėl juos apdoroti gali ir CPU ir GPU.
Tačiau GPU turi daugiau branduolių, kas leidžia atlikti daugiau matematinių veiksmų paralizuotai,
negul CPU. Taip yra pasiekiamas didesnis treniravimo našumas.
\par
Didesniam treniravimo našumui pastebėti naudojant GPU, negu CPU, buvo atliktas paprastas bandymas:
naudojant Google tensorflow struktūrą, apmokame sistemą atpažinti kur yra žmogaus plaučiai kompiuterinės
tomografijos vaizduose su CPU ir GPU. Vidutiniškai apdoruojant 60 megabaitų duomenų, vienam ciklui
tiklas su CPU resursais užtrunka 6 sekunes, o naudojant GPU vidutiniškai vienas žingsnis užtrunka
0.3 sekundės.
\par
% https://www.tensorflow.org/install/gpu
Šiame darbe naudojamas Tensorflow karkasas naudoja NVIDIA CUDA sistemą.
% https://developer.nvidia.com/cuda-zone
CUDA yra paralelinio skaičiavimo platforma, naudojama su grafikos plokštėmis.
% https://developer.nvidia.com/cuda-faq
Ji kiekvieną skaičiavimą paleidžia ant atskiros gijos. Visi duomenys yra suskirstyti
į vienos, dviejų ir trijų dimensijų blokus. Kiekvienams blogas gali turėti virš 512
gijų. Visos gijos kurios operuoja viename bloke dalinasi bendra GPU atmintimi.
Jeigu norima optimizuoti programą ir paleisti jos skaičiavimus ant GPU, tada reikia
susiskaičiuoti kiek vyks paralelinių operacijų ir kiek iš jų dalinsis bendra atmintimi.
Pagal apskaičiuotą kiekį nurodome kiek mes norime turėti blokų ir kiek kiekvienas blokas
turės atskirų gijų. Prieš kiekvieną operaciją, siekiant pasiekti optimalų našumą, jeigu
reikia įkeliame kintamuosiu į bendrą bloko atmintį. Po atliktų skaičiavimų išvestus
rezultatus turime iškelti iš bendros atminties.

% Priekine sklaida
TODO

% Atgaline sklaida
% (http://didattica.cs.unicam.it/lib/exe/fetch.php?media=didattica:magistrale:kebi:ay_1718:ke-11_neural_networks.pdf University of Applied Sciences Northwestern Switzerland)
Atgalinė sklaida vyksta, kai tinklas išveda rezultatą, patikrinama ar
rezultatas yra teisingas ir atitinkamai tinklas reaguoja į tai. Inicijuojant tinklą svoriai yra
nustatomi atsitiktinai. Kiekvienam duomenų rinkiniui yra stebima tinklo išvestis ir tikrinama kokia
ji turėtų būti. Padaryta klaida yra perduodama tinklui atgal, per praeitus sluoksnius, einant nuo
išvesties sluoksnio. Šiame žingsnyje yra naudojama optimizavimo funkcija. Ji parodo kokie turėtų būti
svorių ir tendenciškumo pokyčiai tinklai ir taip yra
keičiami tinklo svoriai ir tendenciškumas atsižvelgiant į padarytą klaidą. Kai klaidos
yra padaromos pakankamai mažos ir jos yra mažesnės už duotą ribinę reikšmę, yra laikoma, kad tinklas
baigė mokymosi procesą.

\section{Aktyavcijos funckijos}

\section{Optimizavimo algoritmai}
\par
Tinklo siekiamas tikslas yra atliekant kuo mažiau mokymosi iteracijų, pasiekti norimą
išvesties tikslumą. Šiam mokymosi greičiui ir tikslumui pasiekti ir naudojami
optimizavimo algoritmai.

\subsection{Gradientinis nuolydis}
% https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6
% https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3
Gradientinio nuolydžio algoritmas
Tai yra pati paprasčiausia optimizavimo funkcija.
\[
\delta = \delta - \alpha * \Delta L (\delta)
\]
Pagal algoritmą yra atliekama daug iteracijų. Kiekvienos iteracijos metu yra
apskaičiuojama delta. Ji yra gaunama iš esamos deltos atėmus mokymosi žingsnio konstanos
ir nuostolių funkcijos gradientą parametro delta atžvilgiu. Kadangi žingsnis kurį
atliekame kiekvienos iteracijos metu yra konstanta, jeigu pradinis parinktas taškas
yra toli nuo lokalaus minimumo, tai prireiks daug iteracijų, kol jis bus pasiektas.
Šiuo metodu taip pat kiekviena iteracija yra peržiūrimi visi duomenys, kad nėra
gerai jeigu duomenų yra itin daug. Tokiu atveju reiktų daug atminties. Šis algoritmas nebus naudojamas, nes nėra
atsižvelgta į praeitas iteracijas ir minimumas yra per lėtai pasiekiamas, algoritmas.

\subsection{Poaibio gradieninis nuolydis}
 Šis algoritmas yra panašus į paprastą gradientinį metodą, tačiau yra kelis pakeitimai.
 Kai yra pereinama per visus duomenis, po kiekvieno duomenų elemento peržiūrėjimo,
 yra atnaujinami parametrai. Todėl po kiekvienos iteracijos, turime visų duomenų
 elementų pakeitimų sudėtį. Todėl minimumas yra pasiekiamas statesne trajektorija ir
 nereikalinga turėti daug atminties. Pagrindinis šio metodo trūkumas yra galimas jo
 lėtumas. Pereiti per visus duomenis užtrunka daug laiko ir nevisos duomenų elementų
 iteracijos prie parametrų pakeitimo daug neprisideda.

\subsection{Mažo poaibio gradientinis nuolydis}
Abiejų minėtų algoritmų idėjų kombinacija išvedė mažo poaibio gradientinį metodą.
Šis algoritmas suskirsto duomenis į poaibius, kurių elementų skaičius yra daugiau nei 1.
Dažniausiai poaibių dydis yra pasirenkamas dvejetas pakeltas kokiu nors laipsniu, nes
vaizdo plokštėm tokius poaibius apdoroja geriau. Šis algoritmas yra greitesnis nei poaibio
gradientinis metodas, nes yra išvengiama mažų iteracijų, kurių duomenų elementas mažai
prisideda prie parametrų pakeitimo. Taip pat šio metodo pasiekiamas tikslumas yra didesnis
nei kitų minėtų algoritmų, nes atsitiktinai parenkami poaibiai skiriasi kiekvieno
poaibių paskirstymo metu, taip sudarant daugiau varijacijų ir daugiau triukšmo. Kadangi
bendrai yra apdorojama daugiau duomenų variacijų, šis metodas gali pasiekti didesnį
tikslumą. Nors šis metodas gali pasiekti didesnį tikslumą ir yra greitesnis.

\subsection{Bendros gradientinių metodų problemos}
\par
Gradientinio nuolydžio metodai yra pirmo laipsnio optimizavimo funckijos, kas
reiškia jog jos naudoja tik pirmo laipsnio išvestines. Kadangi pirma išvestinė rodo
tik funkcijos statumą, bet ne išlenktumą, tai:
\begin{enumerate}
\item Jeigu antra išvestinė yra lygi nuliui, tai funkcija yra tiesinė. Todėl,
mokymosi žingsnis yra lygus alpha.
\item Jeigu antra išvestinė yra didesnė už nulį tai funkcijos išlenktumas eina į viršų.
Todėl funkcijos žingsnis yra mažesnis už mokymosi žingsnį ir optimizavimo funckija gali
diverguoti.
\item Jeigu antra išvestinė yra mažesnė už nulį, tai funkcija yra išlenkta į apačią.
Todėl funkcijos žingsnis yra didesnis už mokymosi žingsnį.
\end{enumerate}
\par
To pasekoje, optimizavimo funkcijos greitis gali būti lėtas arba ji gali diverguoti.

% TODO PASKAITYTI https://arxiv.org/pdf/1710.08402.pdf


%\subsection{}  Algoritmas kuri naudoja tensorflow

\section{Nuostolių funkcijos}

\section{Neuroninių tinklų modeliai}
\par
Šiame darbe yra naudojamas Tensorflow karkaso Mask RCNN inception modelis.
% https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1
\section{Mask RCNN inception}
\par
Mask RCNN modelį sudaro dvi dalys: vaizdo regiono pasiūlymo tinklas (RPN) ir
klasifikavimo modelis. Bendras algoritmas:
\begin{enumerate}
\item Nuotrauka yra praleidžiama per konvoliucinį neuroninį tinklą (RCNN), kuris
sugeneruoja požymių aibę, pagal kurią galima nuspėti, kad tai yra ieškomas objektas.
\end{enumerate}








\section{Tutorial}
\subsection{Poskyris}
Citavimo pavyzdžiai: cituojamas vienas šaltinis \cite{PvzStraipsnLt}; cituojami
keli šaltiniai \cite{PvzStraipsnEn, PvzKonfLt, PvzKonfEn, PvzKnygLt, PvzKnygEn,
PvzElPubLt, PvzElPubEn, PvzMagistrLt, PvzPhdEn}.

\subsubsection{Skirsnis}
\subsubsubsection{Straipsnis}
\subsubsection{Skirsnis}
\section{Skyrius}
\subsection{Poskyris}
\subsection{Poskyris}

\sectionnonum{Išvados}
Išvadose ir pasiūlymuose, nekartojant atskirų dalių apibendrinimų,
suformuluojamos svarbiausios darbo išvados, rekomendacijos bei pasiūlymai.

\printbibliography[heading=bibintoc] % Literatūros šaltiniai aprašomi
% bibliografija.bib faile. Šaltinių sąraše nurodoma panaudota literatūra,
% kitokie šaltiniai. Abėcėlės tvarka išdėstoma tik darbe panaudotų (cituotų,
% perfrazuotų ar bent paminėtų) mokslo leidinių, kitokių publikacijų
% bibliografiniai aprašai (šiuo punktu pasirūpina LaTeX). Aprašai pateikiami
% netransliteruoti.

\appendix  % Priedai
% Prieduose gali būti pateikiama pagalbinė, ypač darbo autoriaus savarankiškai
% parengta, medžiaga. Savarankiški priedai gali būti pateikiami kompiuterio
% diskelyje ar kompaktiniame diske. Priedai taip pat vadinami ir numeruojami.
% Tekstas su priedais siejamas nuorodomis (pvz.: \ref{img:mlp}).

\section{Niauroninio tinklo struktūra}
\begin{figure}[H]
    \centering
    \caption{Paveikslėlio pavyzdys}   % Antraštė įterpiama po paveikslėlio
    \label{img:mlp}
\end{figure}


\section{Eksperimentinio palyginimo rezultatai}
% tablesgenerator.com - converts calculators (e.g. excel) tables to LaTeX
\begin{table}[H]\footnotesize
  \centering
  \caption{Lentelės pavyzdys}    % Antraštė įterpiama prieš lentelę
  {\begin{tabular}{|l|c|c|} \hline
    Algoritmas & $\bar{x}$ & $\sigma^{2}$ \\
    \hline
    Algoritmas A  & 1.6335    & 0.5584       \\
    Algoritmas B  & 1.7395    & 0.5647       \\
    \hline
  \end{tabular}}
  \label{tab:table example}
\end{table}

\end{document}
